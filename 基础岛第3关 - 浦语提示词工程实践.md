

# 基础岛第3关 - 浦语提示词工程实践

## 1. 前期准备工作

准备好开发机，若果没有可以看

https://github.com/InternLM/Tutorial/tree/camp3/docs/L0/Linux

![](https://raw.githubusercontent.com/Chl681006/new-notes/master/20240818170254.png)

### 1.1 环境配置

点击进入开发机，点击Terminal，按照如下脚本创建环境

```python
# 创建虚拟环境
conda create -n langgpt python=3.10 -y
```

激活虚拟环境：

```python
conda activate langgpt
```

安装必要的包：

```python
# 安装一些必要的库
conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=12.1 -c pytorch -c nvidia -y

# 安装其他依赖
pip install transformers==4.43.3

pip install streamlit==1.37.0
pip install huggingface_hub==0.24.3
pip install openai==1.37.1
pip install lmdeploy==0.5.2
```

### 1.2 创建项目路径

```python
# 创建路径
mkdir langgpt
# 进入项目路径
cd langgpt
```

### 1.3 安装必要的软件

```python
apt-get install tmux
```

## 2. 模型部署

这部分基于LMDeploy将开源的InternLM2-chat-1_8b模型部署为OpenAI格式的通用接口。

### 2.1 获取模型

* 如果使用intern-studio开发机，可以直接在路径`/share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b`下找到模型

* 如果不使用开发机，可以从huggingface上获取模型，地址为：https://huggingface.co/internlm/internlm2-chat-1_8b

	可以使用如下脚本下载模型：

	```
	from huggingface_hub import login, snapshot_download
	import os
	
	os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
	
	login(token=“your_access_token")
	
	models = ["internlm/internlm2-chat-1_8b"]
	
	for model in models:
	    try:
	        snapshot_download(repo_id=model,local_dir="langgpt/internlm2-chat-1_8b")
	    except Exception as e:
	        print(e)
	        pass
	```

	

### 2.2 部署模型为OpenAI server



由于服务需要持续运行，需要将进程维持在后台，所以这里使用`tmux`软件创建新的命令窗口。运行如下命令创建窗口：

```
tmux new -t langgpt
```

创建完成后，运行下面的命令进入新的命令窗口(首次创建自动进入，之后需要连接)：

```
tmux a -t langgpt
```

进入命令窗口后，需要在新窗口中再次激活环境，命令参考**0.1节**。然后，使用LMDeploy进行部署，参考如下命令：

使用LMDeploy进行部署，参考如下命令：

```
CUDA_VISIBLE_DEVICES=0 lmdeploy serve api_server /share/new_models/Shanghai_AI_Laboratory/internlm2-chat-1_8b --server-port 23333 --api-keys internlm2
```

更多设置，可以参考：https://lmdeploy.readthedocs.io/en/latest/index.html

部署成功后，可以利用如下脚本调用部署的InternLM2-chat-1_8b模型并测试是否部署成功。

```
from openai import OpenAI

client = OpenAI(
    api_key = "internlm2",
    base_url = "http://0.0.0.0:23333/v1"
)

response = client.chat.completions.create(
    model=client.models.list().data[0].id,
    messages=[
        {"role": "system", "content": "请介绍一下你自己"}
    ]
)

print(response.choices[0].message.content)
```

![](https://raw.githubusercontent.com/Chl681006/new-notes/master/20240818124247.png)

服务启动完成后，可以按Ctrl+B进入`tmux`的控制模式，然后按D退出窗口连接，更多操作[参考](https://aik9.top/)。

### 2.3 图形化界面调用

InternLM部署完成后，可利用提供的`chat_ui.py`创建图形化界面，在实战营项目的tools项目中。

首先，从Github获取项目，运行如下命令：

```
git clone https://github.com/InternLM/Tutorial.git
```

下载完成后，运行如下命令进入项目所在的路径：

```
cd Tutorial/tools
```

进入正确路径后，运行如下脚本运行项目：

```
python -m streamlit run chat_ui.py
```

`chat_ui.py`文件内容：

```Python
import streamlit as st
from openai import OpenAI
import os
import json
import time

# Create a chatbot UI with Streamlit and OpenAI
def chat_ui():
    state = st.session_state
    # Set the title of the app
    st.title("浦语提示词工程实践")
    st.caption("浦语提示词工程实践所用Web UI")

    # Create a client for the OpenAI API
    if "client" not in state:
        st.info("请配置Chatbot的基本设置，其中API Key和Base URL是必须的。")
        pass
    else:
        # if "message_history" not in state:
        #     state.message_history = []
        #     pass
        # if "system_prompt" in state:
        #     state.message_history.append({"role": "system", "content": state.system_prompt})
        user_input = st.chat_input("输入消息")
        if user_input:
            state.message_history.append({"role": "user", "content": user_input})
            # Generate a response from the chatbot
            if "max_tokens" in state:
                response = state.client.chat.completions.create(
                    model=state.client.models.list().data[0].id,
                    messages=state.message_history,
                    max_tokens=state.max_tokens,
                    temperature=state.temperature
                )
            else:
                response = state.client.chat.completions.create(
                    model=state.client.models.list().data[0].id,
                    messages=state.message_history,
                    temperature=state.temperature
                )
            state.message_history.append({"role": "assistant", "content": response.choices[0].message.content})
            pass
        for message in state.message_history:
            if message["role"] == "system":
                continue
            else:
                st.chat_message(message["role"]).write(message["content"])

    # Create a text input for the user to type their message

    pass
# define a side bar for the setting of the chatbot, such as the max token length, temperature, api_key, base_url, system prompt, etc.
def side_bar():
    st.sidebar.title("设置")
    state = st.session_state
    # Set a form of the settings
    with st.sidebar.form(key="settings"):
        # Set the max token length for the chatbot
        max_tokens = st.number_input("最大token长度", min_value=0, max_value=2048, value=100, step=1)
        # Set the temperature for the chatbot
        temperature = st.number_input("Temperature", min_value=0.0, max_value=1.0, value=0.0, step=0.01)
        # Set the api key for the OpenAI API
        api_key = st.text_input("API Key", value="internlm2")
        # Set the base url for the OpenAI API
        base_url = st.text_input("Base URL",value="http://0.0.0.0:23333/v1")
        # Set the system prompt for the chatbot
        system_prompt = st.text_area("系统提示", value="")
        # Add a submit button to the form
        submit = st.form_submit_button("保存设置")
        # If the submit button is pressed, save the settings
        if submit:
            if max_tokens != 0:
                state.max_tokens = max_tokens
            state.temperature = temperature
            state.api_key = api_key
            state.base_url = base_url
            state.message_history = []
            if system_prompt != "":
                state.system_prompt = system_prompt
                state.message_history.append({"role": "system", "content": system_prompt})
            state.client = OpenAI(api_key=state.api_key, base_url=state.base_url)
            pass
    if st.sidebar.button("开启新对话"):
        if not os.path.exists("chat_history"):
            os.mkdir("chat_history")
            pass
        with open(f"chat_history/{time.time()}.json", "w") as f:
            json.dump(state.message_history, f, ensure_ascii=False)
            pass
        state.message_history = []
        st.rerun()

    pass

if __name__ == "__main__":
    side_bar()
    chat_ui()
    pass
```

参考[L0/Linux的2.3部分](https://github.com/InternLM/Tutorial/tree/camp3/docs/L0/Linux#23-端口映射)进行端口映射，在本地终端中输入映射命令，可以参考如下命令：

```
ssh -p {ssh端口，从InternStudio获取} root@ssh.intern-ai.org.cn -CNg -L 7860:127.0.0.1:8501 -o StrictHostKeyChecking=no
```



如果未配置开发机公钥，还需要输入密码，从InternStudio获取。上面这一步是将开发机上的8501(web界面占用的端口)映射到本地机器的端口，之后可以访问http://localhost:7860/打开界面。

启动后界面如下：

![](https://raw.githubusercontent.com/Chl681006/new-notes/master/20240818124838.png)

左侧边栏为对话的部分设置，其中最大token长度设置为0时表示不限制生成的最大token长度。API Key和Base URL是部署InternLM时的设置，必须填写。在保存设置之后，可以启动对话界面：

![](https://raw.githubusercontent.com/Chl681006/new-notes/master/20240818124942.png)

若要控制模型执行某些具体的特殊任务，也可于左侧边栏设置系统提示。

## 3. 提示工程(Prompt Engineering)



### 3.1 什么是Prompt



Prompt是一种用于指导以大语言模型为代表的**生成式人工智能**生成内容(文本、图像、视频等)的输入方式。它通常是一个简短的文本或问题，用于描述任务和要求。

Prompt可以包含一些特定的关键词或短语，用于引导模型生成符合特定主题或风格的内容。例如，如果我们要生成一篇关于“人工智能”的文章，我们可以使用“人工智能”作为Prompt，让模型生成一篇关于人工智能的介绍、应用、发展等方面的文章。

Prompt还可以包含一些特定的指令或要求，用于控制生成文本的语气、风格、长度等方面。例如，我们可以使用“请用幽默的语气描述人工智能的发展历程”作为Prompt，让模型生成一篇幽默风趣的文章。

总之，Prompt是一种灵活、多样化的输入方式，可以用于指导大语言模型生成各种类型的内容。

[![img](https://camo.githubusercontent.com/68b151a6aac3b965c52ed41954f10c0f71e5928946096a77e5b1e11cca93f867/68747470733a2f2f66696c65732e6d646e6963652e636f6d2f757365722f35363330362f32626464383163352d623366382d346365642d623366382d3761623437316563313165382e706e67)](https://camo.githubusercontent.com/68b151a6aac3b965c52ed41954f10c0f71e5928946096a77e5b1e11cca93f867/68747470733a2f2f66696c65732e6d646e6963652e636f6d2f757365722f35363330362f32626464383163352d623366382d346365642d623366382d3761623437316563313165382e706e67)

###  3.2 什么是提示工程



提示工程是一种通过设计和调整输入(Prompts)来改善模型性能或控制其输出结果的技术。

在模型回复的过程中，首先获取用户输入的文本，然后处理文本特征并根据输入文本特征预测之后的文本，原理为**next token prediction**。

提示工程是模型性能优化的基石，有以下六大基本原则：

* 指令要清晰
* 提供参考内容
* 复杂的任务拆分成子任务
* 给 LLM“思考”时间(给出过程)
* 使用外部工具
* 系统性测试变化

### 3.3 提示设计框架



* CRISPE，参考：https://github.com/mattnigh/ChatGPT3-Free-Prompt-List

	* **C**apacity and **R**ole (能力与角色)：希望 ChatGPT 扮演怎样的角色。
	* **I**nsight (洞察力)：背景信息和上下文(坦率说来我觉得用 Context 更好)
	* **S**tatement (指令)：希望 ChatGPT 做什么。
	* **P**ersonality (个性)：希望 ChatGPT 以什么风格或方式回答你。
	* **E**xperiment (尝试)：要求 ChatGPT 提供多个答案。

	写出的提示如下：

	```
	Act as an expert on software development on the topic of machine learning frameworks, and an expert blog writer. The audience for this blog is technical professionals who are interested in learning about the latest advancements in machine learning. Provide a comprehensive overview of the most popular machine learning frameworks, including their strengths and weaknesses. Include real-life examples and case studies to illustrate how these frameworks have been successfully used in various industries. When responding, use a mix of the writing styles of Andrej Karpathy, Francois Chollet, Jeremy Howard, and Yann LeCun.
	```

	

* CO-STAR，参考：https://aiadvisoryboards.wordpress.com/2024/01/30/co-star-framework/

	[![img](https://camo.githubusercontent.com/1bf99752a38dd8d9577ba36ea1912c141981f47b1dce8b88ecacdeb7cfe1b64d/68747470733a2f2f66696c65732e6d646e6963652e636f6d2f757365722f35363330362f61623834623464302d383433632d343630622d393235612d3136653037666566633539352e706e67)](https://camo.githubusercontent.com/1bf99752a38dd8d9577ba36ea1912c141981f47b1dce8b88ecacdeb7cfe1b64d/68747470733a2f2f66696c65732e6d646e6963652e636f6d2f757365722f35363330362f61623834623464302d383433632d343630622d393235612d3136653037666566633539352e706e67)

	* **C**ontext (背景): 提供任务背景信息
	* **O**bjective (目标): 定义需要LLM执行的任务
	* **S**tyle (风格): 指定希望LLM具备的写作风格
	* **T**one (语气): 设定LLM回复的情感基调
	* **A**udience (观众): 表明回复的对象
	* **R**esponse (回复): 提供回复格式

	完成的提示如下：

	```
	# CONTEXT # 
	I am a personal productivity developer. In the realm of personal development and productivity, there is a growing demand for systems that not only help individuals set goals but also convert those goals into actionable steps. Many struggle with the transition from aspirations to concrete actions, highlighting the need for an effective goal-to-system conversion process.
	
	#########
	
	# OBJECTIVE #
	Your task is to guide me in creating a comprehensive system converter. This involves breaking down the process into distinct steps, including identifying the goal, employing the 5 Whys technique, learning core actions, setting intentions, and conducting periodic reviews. The aim is to provide a step-by-step guide for seamlessly transforming goals into actionable plans.
	
	#########
	
	# STYLE #
	Write in an informative and instructional style, resembling a guide on personal development. Ensure clarity and coherence in the presentation of each step, catering to an audience keen on enhancing their productivity and goal attainment skills.
	
	#########
	
	# Tone #
	Maintain a positive and motivational tone throughout, fostering a sense of empowerment and encouragement. It should feel like a friendly guide offering valuable insights.
	
	# AUDIENCE #
	The target audience is individuals interested in personal development and productivity enhancement. Assume a readership that seeks practical advice and actionable steps to turn their goals into tangible outcomes.
	
	#########
	
	# RESPONSE FORMAT #
	Provide a structured list of steps for the goal-to-system conversion process. Each step should be clearly defined, and the overall format should be easy to follow for quick implementation. 
	
	#############
	
	# START ANALYSIS #
	If you understand, ask me for my goals.
	```

	

## 4. LangGPT结构化提示词



LangGPT 是 **Language For GPT-like LLMs** 的简称，中文名为结构化提示词。LangGPT 是一个帮助你编写高质量提示词的工具，理论基础是我们提出的一套模块化、标准化的提示词编写方法论——结构化提示词。我们希望揭开提示工程的神秘面纱，为大众提供一套可操作、可复现的提示词方法论、工具和交流社群。我们的愿景是让人人都能写出高质量提示词。LangGPT社区文档：[https://langgpt.ai](https://langgpt.ai/)

### 4.1 LangGPT结构



LangGPT框架参考了面向对象程序设计的思想，设计为基于角色的双层结构，一个完整的提示词包含**模块-内部元素**两级，模块表示要求或提示LLM的方面，例如：背景信息、建议、约束等。内部元素为模块的组成部分，是归属某一方面的具体要求或辅助信息，分为赋值型和方法型。

[![img](https://camo.githubusercontent.com/3c5e6fe339536b1ac06df256b6fabe82b126f4d9eec98068e1fc490d9e7686bb/68747470733a2f2f66696c65732e6d646e6963652e636f6d2f757365722f35363330362f39316331646666652d303131642d343933322d623662322d3138373238346130616439312e706e67)](https://camo.githubusercontent.com/3c5e6fe339536b1ac06df256b6fabe82b126f4d9eec98068e1fc490d9e7686bb/68747470733a2f2f66696c65732e6d646e6963652e636f6d2f757365722f35363330362f39316331646666652d303131642d343933322d623662322d3138373238346130616439312e706e67)

### 4.2 编写技巧



* **构建全局思维链**

	对大模型的 Prompt 应用CoT 思维链方法的有效性是被研究和实践广泛证明了的。首先可以根据场景选择基本的模块。

	[![img](https://camo.githubusercontent.com/ccfb3296bfe8910486f82012f533c45b756173ee5d58f73ce5407760c8546188/68747470733a2f2f66696c65732e6d646e6963652e636f6d2f757365722f35363330362f30356533383061382d623632372d343266322d623065362d3334336263393932336633652e706e67)](https://camo.githubusercontent.com/ccfb3296bfe8910486f82012f533c45b756173ee5d58f73ce5407760c8546188/68747470733a2f2f66696c65732e6d646e6963652e636f6d2f757365722f35363330362f30356533383061382d623632372d343266322d623065362d3334336263393932336633652e706e67)

	**一个好的结构化 Prompt 模板，某种意义上是构建了一个好的全局思维链。** 如 LangGPT 中展示的模板设计时就考虑了如下思维链:

	> 💡 Role (角色) -> Profile（角色简介）—> Profile 下的 skill (角色技能) -> Rules (角色要遵守的规则) -> Workflow (满足上述条件的角色的工作流程) -> Initialization (进行正式开始工作的初始化准备) -> 开始实际使用

	一个好的 Prompt ，内容结构上最好也是逻辑清晰连贯的。**结构化 prompt 方法将久经考验的逻辑思维链路融入了结构中，大大降低了思维链路的构建难度。**

	构建 Prompt 时，不妨参考优质模板的全局思维链路，熟练掌握后，完全可以对其进行增删改留调整得到一个适合自己使用的模板。例如当你需要控制输出格式，尤其是需要格式化输出时，完全可以增加 `Ouput` 或者 `OutputFormat` 这样的模块。

* **保持上下文语义一致性**

	包含两个方面，一个是**格式语义一致性**，一个是**内容语义一致性**。

	**格式语义一致性是指标识符的标识功能前后一致。** 最好不要混用，比如 `#` 既用于标识标题，又用于标识变量这种行为就造成了前后不一致，这会对模型识别 Prompt 的层级结构造成干扰。

	**内容语义一致性是指思维链路上的属性词语义合适。** 例如 LangGPT 中的 `Profile` 属性词，使之功能更加明确：即角色的简历。结构化 Prompt 思想被广泛使用后衍生出了许许多多的模板，但基本都保留了 `Profile` 的诸多设计，说明其设计是成功有效的。

	**内容语义一致性还包括属性词和相应模块内容的语义一致。** 例如 `Rules` 部分是角色需要遵守规则，则不宜将角色技能、描述大量堆砌在此。

* **有机结合其他 Prompt 技巧**

	LangGPT结构在设计时没有拘泥于具体的方面，相比其他的提示设计框架，更加灵活，具有更强的可扩展性和兼容性，可以很好地结合其他提示设计技巧。

	构建高质量 Prompt 时，将这些方法结合使用，结构化方式能够更便于各个技巧间的协同组织，例如将 CoT 方法融合到结构化 Prompt 中编写提示词。 汇总现有的一些方法：

	1. 细节法：给出更清晰的指令，包含更多具体的细节
	2. 分解法：将复杂的任务分解为更简单的子任务 （Let's think step by step, CoT，LangChain等思想）
	3. 记忆法：构建指令使模型时刻记住任务，确保不偏离任务解决路径（system 级 prompt）
	4. 解释法：让模型在回答之前进行解释，说明理由 （CoT 等方法）
	5. 投票法：让模型给出多个结果，然后使用模型选择最佳结果 （ToT 等方法）
	6. 示例法：提供一个或多个具体例子，提供输入输出示例 （one-shot, few-shot 等方法）

	上面这些方法最好结合使用，以实现在复杂任务中实现使用不可靠工具（LLMs）构建可靠系统的目标。

## 5. 浦语提示词工程实践(LangGPT版)



编写完LangGPT提示词后，可以将其作为系统提示，也可直接作为交互式对话的输入。**推荐作为系统提示**。

[![img](https://camo.githubusercontent.com/53d5f30f58c6efb39b0420c0381d44b81ef92e282a066c0279ebb0d768f51c60/68747470733a2f2f66696c65732e6d646e6963652e636f6d2f757365722f35363330362f33323339663063382d383365632d343934332d396362302d3838306135343861333231662e706e67)](https://camo.githubusercontent.com/53d5f30f58c6efb39b0420c0381d44b81ef92e282a066c0279ebb0d768f51c60/68747470733a2f2f66696c65732e6d646e6963652e636f6d2f757365722f35363330362f33323339663063382d383365632d343934332d396362302d3838306135343861333231662e706e67)

填入系统提示后保存设置，之后可以与自定义的助手角色进行对话。

```
# Role: 数字大小比较专家

## Profile
- author: LangGPT
- version: 1.0
- language: 中文
- description: 你是一位擅长比较数字大小的专家，能够快速准确地判断和比较不同数字的大小，并解释其背后的逻辑和方法。

## Skills
1. 精通数字大小的比较方法。
2. 能够清晰地解释比较过程和结果。
3. 具备解决复杂数字比较问题的能力。
4. 能够根据不同场景提供个性化的比较策略。

## Background
在日常生活和工作中，我们经常需要比较不同数字的大小，例如在数据分析、财务管理、竞赛评分等领域。准确的数字比较能够帮助我们做出更好的决策。

## Goals
1. 帮助用户快速准确地比较数字大小。
2. 提供数字比较的深入理解和应用。
3. 提升用户的数字比较技能和效率。

## Constraints
1. 确保提供的比较结果准确无误。
2. 解释过程要清晰易懂，适合用户的理解水平。
3. 尊重用户的隐私和学习节奏。

## Workflows
1. 用户提出具体的数字比较问题或需求。
2. 分析数字，确定比较方法。
3. 提供详细的比较过程和结果解释。
4. 根据用户反馈调整比较策略或解释方法。

## Initialization
欢迎使用数字大小比较专家提示词生成器。请描述您需要比较的数字及其具体场景，我将为您提供专业的比较方法和解释。
```

这种方式会出错，比如比较11.22和11.23的大小，对提示词进行改善以后：

```
# Role: 数字大小比较专家

## Profile
- author: LangGPT
- version: 1.3
- language: 中文
- description: 你是一位擅长比较数字大小的专家，能够快速准确地判断和比较不同数字的大小，并解释其背后的逻辑和方法。

## Skills
1. 精通数字大小的比较方法，包括逐位比较和小数点后位数比较。
2. 能够清晰地解释比较过程和结果。
3. 具备解决复杂数字比较问题的能力。
4. 能够根据不同场景提供个性化的比较策略。

## Background
在日常生活和工作中，我们经常需要比较不同数字的大小，例如在数据分析、财务管理、竞赛评分等领域。准确的数字比较能够帮助我们做出更好的决策。

## Goals
1. 帮助用户快速准确地比较数字大小。
2. 提供数字比较的深入理解和应用。
3. 提升用户的数字比较技能和效率。

## Constraints
1. 确保提供的比较结果准确无误。
2. 解释过程要清晰易懂，适合用户的理解水平。
3. 尊重用户的隐私和学习节奏。

## Workflows
1. 用户提出具体的数字比较问题或需求。
2. 分析数字，确定比较方法。
3. 提供详细的比较过程和结果解释。
4. 根据用户反馈调整比较策略或解释方法。

## Initialization
欢迎使用数字大小比较专家提示词生成器。请描述您需要比较的数字及其具体场景，我将为您提供专业的比较方法和解释。

## 详细比较方法

### 逐位比较
1. 从整数部分开始，逐位比较两个数字的每一位。
2. 如果某一位上的数字不同，则较大的数字整体较大。
3. 如果整数部分相同，则继续比较小数部分。

### 小数点后位数比较
1. 在整数部分和前几位小数相同的情况下，比较小数点后位数。
2. 如果一个数字的小数点后位数比另一个数字多，则该数字较大。
3. 如果小数点后位数相同，则继续逐位比较小数部分。

## 示例

### 比较数字12.8和12.88大小

1. **逐位比较**：
   - 整数部分都是12，相同。
   - 小数点后第一位，12.8是8，12.88是8，相同。
   - 小数点后第二位，12.8没有第二位，12.88是8。
   - 因此，12.88更大。

综上所述，数字12.88大于数字12.8。
```

![](https://raw.githubusercontent.com/Chl681006/new-notes/master/20240818125921.png)
